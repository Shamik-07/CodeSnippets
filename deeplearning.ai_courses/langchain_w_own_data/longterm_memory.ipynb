{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identify if question is relevant to be stored in Long-Term-Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradeQuestion(BaseModel):\n",
    "    \"\"\"Boolean value to check whether a question is related to the specified topics.\"\"\"\n",
    "\n",
    "    score: str = Field(\n",
    "        description=\"Is the question relevant? Respond with 'Yes' or 'No'.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"\"\"\n",
    "You are a classifier that examines the given question or statement for any personal information or preferences.\n",
    "Your job is to determine whether the input contains:\n",
    "1. Personal information about the user (e.g., name, occupation, location, contact details, or other identifiable information).\n",
    "2. Preferences, habits, or any explicitly mentioned likes/dislikes.\n",
    "\n",
    "If you find any such information, respond with 'Yes'. If the input does not contain any personal information or preferences, respond with 'No'.\n",
    "Your response must be ONLY 'Yes' or 'No'.\n",
    "\"\"\"\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Here is the input: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "grader_llm = grade_prompt | structured_llm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_llm.invoke({\"question\": \"Where is Thomas Müller from?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grader_llm.invoke({\"question\": \"Where is Thomas Müller from? I love playing football myself\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarise question/information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Where is Thomas Müller from? I love playing football myself.\"\n",
    "\n",
    "system = \"\"\"\n",
    "You are an extractor focusing on personal information **about the user**.\n",
    "Ignore references to third parties unless they somehow reveal personal details **about the user**.\n",
    "\n",
    "Personal information (for the user) includes:\n",
    "1. The user’s own name or identifying details.\n",
    "2. The user’s locations (e.g., \"I live in Seattle\").\n",
    "3. The user’s hobbies, preferences, or habits (\"I love playing soccer every day\").\n",
    "4. Any unique personal details shared by the user.\n",
    "\n",
    "If the user mentions others’ personal info (like celebrity data), do not include it.\n",
    "\n",
    "### Few-Shot Examples:\n",
    "\n",
    "User: \"I am John, I live in Berlin.\"\n",
    "- Extract only user info: \"User name: John, location: Berlin.\"\n",
    "\n",
    "User: \"Do you know where Beyoncé is from? I love dancing to her songs every night!\"\n",
    "- Do NOT extract Beyoncé’s info. Instead, focus on the user’s statement:\n",
    "  \"User enjoys dancing to Beyoncé's songs every night.\"\n",
    "\n",
    "User: \"Hello, how are you?\"\n",
    "- No personal info to extract. Output: \"No personal info found.\"\n",
    "\n",
    "Now read the message carefully and respond with the user’s personal info (if any). Otherwise, respond \"No personal info found.\"\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", f\"Extract and summarize personal information: {message}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "summarizer = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = summarizer.invoke({})\n",
    "print(f\"Extracted Personal Information: {result}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated, Literal, TypedDict, Sequence\n",
    "from pydantic import BaseModel, Field\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "import uuid\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "store = InMemoryStore()\n",
    "USER_ID = \"user-123\"\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    personal_info_detected: str\n",
    "    personal_info_extracted: str\n",
    "    is_duplicate: str\n",
    "    collected_memories: str\n",
    "\n",
    "class GradeQuestion(BaseModel):\n",
    "    score: str = Field()\n",
    "\n",
    "def personal_info_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Classifies if the last user message contains personal info.\n",
    "    Uses few-shot examples to clarify what \"personal info\" is.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    system_prompt = \"\"\"You are a classifier that determines if a message contains personal info.\n",
    "Personal info may include:\n",
    "- Names (e.g., \"John Smith\"),\n",
    "- Locations (e.g., \"Berlin\", \"123 Main St\"),\n",
    "- Preferences / hobbies (\"I love playing soccer\", \"I prefer short responses\"),\n",
    "- Occupation details, phone number, or any unique ID.\n",
    "\n",
    "Examples:\n",
    "User: \"My name is Thomas, I live in Vancouver.\"\n",
    "Classifier: \"Yes\"\n",
    "\n",
    "User: \"I love pizza with extra cheese.\"\n",
    "Classifier: \"Yes\"  (because it expresses a personal preference)\n",
    "\n",
    "User: \"Hello, how are you?\"\n",
    "Classifier: \"No\"  (no personal info)\n",
    "\n",
    "User: \"This is great weather.\"\n",
    "Classifier: \"No\"  (no personal info)\n",
    "\n",
    "Now analyze the new user message. Respond ONLY 'Yes' or 'No'.\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_prompt),\n",
    "            (\"human\", \"{message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    structured_llm = llm.with_structured_output(GradeQuestion)\n",
    "    chain = prompt | structured_llm\n",
    "\n",
    "    result = chain.invoke({\"message\": message})\n",
    "    state[\"personal_info_detected\"] = result.score.strip()\n",
    "    return state\n",
    "\n",
    "def personal_info_router(state: AgentState) -> Literal[\"extract_personal_info\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If classified 'Yes', go to extraction. Otherwise skip directly to retrieving memories.\n",
    "    \"\"\"\n",
    "    if state[\"personal_info_detected\"].lower() == \"yes\":\n",
    "        return \"extract_personal_info\"\n",
    "    return \"retrieve_memories\"\n",
    "\n",
    "def personal_info_extractor(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Extracts personal info from the user's message using few-shot examples.\n",
    "    \"\"\"\n",
    "    message = state[\"messages\"][-1].content\n",
    "\n",
    "    # A few-shot style system prompt for extraction:\n",
    "    extractor_system = \"\"\"You are an extractor focusing on personal info.\n",
    "\n",
    "Examples:\n",
    "User: \"I am John and I live in Seattle.\"\n",
    "You output: \"User name: John, Location: Seattle.\"\n",
    "\n",
    "User: \"Hey, I'm Lucy. I love playing guitar!\"\n",
    "You output: \"User name: Lucy, Hobby: playing guitar.\"\n",
    "\n",
    "User: \"Just a random statement about the weather.\"\n",
    "You output: \"No personal info found.\"\n",
    "\n",
    "Now read the input and extract personal info as a single-sentence summary:\n",
    "- If there's name, location, preferences, or any unique details, mention them briefly.\n",
    "- If no personal info is found, write: 'No personal info found.'\"\"\"\n",
    "\n",
    "    extractor_prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", extractor_system),\n",
    "            (\"human\", \"Input: {message}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    chain = extractor_prompt | llm\n",
    "    extracted_info = chain.invoke({\"message\": message})\n",
    "\n",
    "    state[\"personal_info_extracted\"] = extracted_info.content.strip()\n",
    "    return state\n",
    "\n",
    "class InfoNoveltyGrade(BaseModel):\n",
    "    score: str = Field()\n",
    "\n",
    "def personal_info_duplicate_classifier(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Checks if the newly extracted info is already in the store or not.\n",
    "    If 'Yes', it's new info. If 'No', it's a duplicate.\n",
    "    \"\"\"\n",
    "    new_info = state.get(\"personal_info_extracted\", \"\")\n",
    "    namespace = (\"memories\", USER_ID)\n",
    "    results = store.search(namespace)\n",
    "    old_info_list = [doc.value[\"data\"] for doc in results]\n",
    "\n",
    "    system_msg = \"\"\"You are a classifier that checks if the new personal info is already stored.\n",
    "If the new info adds anything new, respond 'Yes'. Otherwise 'No'.\"\"\"\n",
    "\n",
    "    old_info_str = \"\\n\".join(old_info_list) if old_info_list else \"No stored info so far.\"\n",
    "    human_template = \"\"\"New info:\\n{new_info}\\n\n",
    "Existing memory:\\n{old_info}\\n\n",
    "Answer ONLY 'Yes' if the new info is unique. Otherwise 'No'.\"\"\"\n",
    "    human_msg = human_template.format(new_info=new_info, old_info=old_info_str)\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", system_msg),\n",
    "            (\"human\", \"{human_msg}\"),\n",
    "        ]\n",
    "    )\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\").with_structured_output(InfoNoveltyGrade)\n",
    "    chain = prompt | llm\n",
    "    result = chain.invoke({\"human_msg\": human_msg})\n",
    "    state[\"is_duplicate\"] = result.score.strip()\n",
    "    return state\n",
    "\n",
    "def personal_info_deduper_router(state: AgentState) -> Literal[\"personal_info_storer\", \"retrieve_memories\"]:\n",
    "    \"\"\"\n",
    "    If 'Yes', store the new info. Otherwise skip storing.\n",
    "    \"\"\"\n",
    "    if state[\"is_duplicate\"].lower() == \"yes\":\n",
    "        return \"personal_info_storer\"\n",
    "    return \"retrieve_memories\"\n",
    "\n",
    "def personal_info_storer(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Stores the new personal info in memory if it exists.\n",
    "    \"\"\"\n",
    "    extracted = state.get(\"personal_info_extracted\")\n",
    "    if extracted:\n",
    "        namespace = (\"memories\", USER_ID)\n",
    "        store.put(namespace, str(uuid.uuid4()), {\"data\": extracted})\n",
    "    return state\n",
    "\n",
    "def retrieve_memories(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Retrieves all personal info from the store and aggregates into 'collected_memories'.\n",
    "    \"\"\"\n",
    "    namespace = (\"memories\", USER_ID)\n",
    "    results = store.search(namespace)\n",
    "    memory_strs = [doc.value[\"data\"] for doc in results]\n",
    "    state[\"collected_memories\"] = \"\\n\".join(memory_strs)\n",
    "    return state\n",
    "\n",
    "def log_personal_memory(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Logs the memory to stdout for debugging (optional).\n",
    "    \"\"\"\n",
    "    print(\"----- Logging Personal Memory -----\")\n",
    "    if state[\"collected_memories\"]:\n",
    "        for i, line in enumerate(state[\"collected_memories\"].split(\"\\n\"), start=1):\n",
    "            print(f\"[Memory {i}] {line}\")\n",
    "    else:\n",
    "        print(\"[Memory] No personal info stored yet.\")\n",
    "    return state\n",
    "\n",
    "def call_model(state: AgentState) -> AgentState:\n",
    "    \"\"\"\n",
    "    Final LLM call that uses the collected memories in a SystemMessage.\n",
    "    \"\"\"\n",
    "    personal_info = state.get(\"collected_memories\", \"\")\n",
    "    system_msg = SystemMessage(\n",
    "        content=f\"You are a helpful assistant. The user has shared these personal details:\\n{personal_info}\"\n",
    "    )\n",
    "    all_messages = [system_msg] + list(state[\"messages\"])\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    response = llm.invoke(all_messages)\n",
    "    state[\"messages\"] = state[\"messages\"] + [response]\n",
    "    return state\n",
    "\n",
    "workflow = StateGraph(AgentState)\n",
    "workflow.add_node(\"personal_info_classifier\", personal_info_classifier)\n",
    "workflow.add_node(\"personal_info_extractor\", personal_info_extractor)\n",
    "workflow.add_node(\"personal_info_duplicate_classifier\", personal_info_duplicate_classifier)\n",
    "workflow.add_node(\"personal_info_storer\", personal_info_storer)\n",
    "workflow.add_node(\"retrieve_memories\", retrieve_memories)\n",
    "workflow.add_node(\"log_personal_memory\", log_personal_memory)\n",
    "workflow.add_node(\"call_model\", call_model)\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_classifier\",\n",
    "    personal_info_router,\n",
    "    {\n",
    "        \"extract_personal_info\": \"personal_info_extractor\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_extractor\", \"personal_info_duplicate_classifier\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"personal_info_duplicate_classifier\",\n",
    "    personal_info_deduper_router,\n",
    "    {\n",
    "        \"personal_info_storer\": \"personal_info_storer\",\n",
    "        \"retrieve_memories\": \"retrieve_memories\",\n",
    "    },\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"personal_info_storer\", \"retrieve_memories\")\n",
    "workflow.add_edge(\"retrieve_memories\", \"log_personal_memory\")\n",
    "workflow.add_edge(\"log_personal_memory\", \"call_model\")\n",
    "workflow.add_edge(\"call_model\", END)\n",
    "\n",
    "workflow.set_entry_point(\"personal_info_classifier\")\n",
    "\n",
    "checkpointer = MemorySaver()\n",
    "graph = workflow.compile(checkpointer=checkpointer, store=store)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image, display\n",
    "from langchain_core.runnables.graph import MermaidDrawMethod\n",
    "\n",
    "display(\n",
    "    Image(\n",
    "        graph.get_graph().draw_mermaid_png(\n",
    "            draw_method=MermaidDrawMethod.API,\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_1 = {\"messages\": [HumanMessage(content=\"Hi, I'm Thomas Müller and I love playing football.\")]}\n",
    "graph.invoke(input=input_data_1, config={\"configurable\": {\"thread_id\": 99}})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_2 = {\"messages\": [HumanMessage(content=\"Great!\")]}\n",
    "graph.invoke(input=input_data_2, config={\"configurable\": {\"thread_id\": 99}})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_3 = {\"messages\": [HumanMessage(content=\"What do you know about me?\")]}\n",
    "graph.invoke(input=input_data_3, config={\"configurable\": {\"thread_id\": 2}})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
