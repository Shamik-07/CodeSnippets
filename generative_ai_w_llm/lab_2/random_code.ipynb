{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb644e2-8589-44f1-bb22-d43938cbf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_traveller(m,n, d={}):\n",
    "    key = (m,n)\n",
    "    if m==1 and n==1:\n",
    "        return 1\n",
    "    if m==0 or n==0:\n",
    "        return 0\n",
    "    if key in d:\n",
    "        return d[key]\n",
    "    d[key]=grid_traveller(m-1,n,d) + grid_traveller(m,n-1,d)\n",
    "    return d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca01adaa-9723-4f7a-b212-d5dc4f5b2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 2333606220)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_traveller(2,3), grid_traveller(3,2), grid_traveller(1,1), grid_traveller(18, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb84088-02c2-4483-ac73-0b5bad24d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n, d={}):\n",
    "    if n<=2:\n",
    "        return 1\n",
    "    if n in d:\n",
    "        return d[n]\n",
    "    d[n] = fib(n-1, d) + fib(n-2, d)\n",
    "    return d[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ceef4c-0807-4ff2-aa1f-fd500e8f2e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fib(3), fib(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6f0f2ff-d9b8-4a8b-bb90-359e6cf4b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alpha = \"The quick brown foxes jumped over the lazy dog.\"\n",
    "all_alpha = all_alpha.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6311f06e-e9fb-4e94-8331-5ef08025947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('The', 'quick', 'brown', 'foxes'),\n",
       " ('quick', 'brown', 'foxes', 'jumped'),\n",
       " ('brown', 'foxes', 'jumped', 'over'),\n",
       " ('foxes', 'jumped', 'over', 'the'),\n",
       " ('jumped', 'over', 'the', 'lazy'),\n",
       " ('over', 'the', 'lazy', 'dog.'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(zip(*[all_alpha[i:] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23778131-e0b1-4ea6-b31b-3bb3bdafbca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given an array of strings, group the anagrams together. You can return the answer in any order.\n",
    "\n",
    "An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\n",
    "\n",
    "Solved using hashmap\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"] Output: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]] Example 2:\n",
    "\n",
    "Input: strs = [\"\"] Output: [[\"\"]] Example 3:\n",
    "\n",
    "Input: strs = [\"a\"] Output: [[\"a\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f37cf21e-a863-47ff-9d58-0d3d298a9067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'c', 't')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(sorted(\"atc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55b148bc-98b7-474d-965a-2fb88c0e7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana(strs):\n",
    "    d={}\n",
    "    for eachstr in strs:\n",
    "        d.setdefault(tuple(sorted(eachstr)),[]).append(eachstr)\n",
    "    return d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "799c4fa2-7d32-4e95-8e8b-086298a7732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['eat', 'tea', 'ate'], ['tan', 'nat'], ['bat']])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana([\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d373ae-fa60-4422-879e-8aaf5b1ea8b4",
   "metadata": {},
   "source": [
    "Can Sum O(m*n)\n",
    "\n",
    "Given a target sum and a list of integers, verify whether the integers can actually match the target sum, same integers can be reused as many times as needed.\n",
    "\n",
    "Example 1 target = 7 list = [5,4,3,7] output = True\n",
    "\n",
    "Example 2 target = 14 list = [5,5] output = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af719287-7c2d-4dba-9796-80b9e497310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cansum(target, ints, d={}):\n",
    "    if target in d:\n",
    "        return d[target]\n",
    "    if target ==0:\n",
    "        return True\n",
    "    if target < 0:\n",
    "        return False\n",
    "    for i in ints:\n",
    "        rem = target - i\n",
    "        if cansum(rem, ints, d):\n",
    "            d[target] = True\n",
    "            return True\n",
    "    d[target]= False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9d68ce73-332a-458b-94fc-83c4ff0ca83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cansum(7, [5,4,3,7], {}), cansum(7, [2,4], {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac9428-b448-4818-a90d-646ff228c37e",
   "metadata": {},
   "source": [
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "You can return the answer in any order.\n",
    "\n",
    "Solved using hashmap\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [2,7,11,15], target = 9 Output: [0,1] Explanation: Because nums[0] + nums[1] == 9, we return [0, 1]. Example 2:\n",
    "\n",
    "Input: nums = [3,2,4], target = 6 Output: [1,2] Example 3:\n",
    "\n",
    "Input: nums = [3,3], target = 6 Output: [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e4f1f4e-1feb-44c0-91dd-709a01dc6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosum(ls, t):\n",
    "    d={}\n",
    "    for i,j in enumerate(ls):\n",
    "        rem = t-j\n",
    "        if rem in d:\n",
    "            return [d[rem],i]\n",
    "        d[j]=i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81947585-d4ff-4380-a9d9-fe9041c128de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosum([3,3],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95747831-465c-41c9-931d-67a6d21c1325",
   "metadata": {},
   "source": [
    "Pivot Index\n",
    "\n",
    "Given an array of integers nums, calculate the pivot index of this array.\n",
    "\n",
    "The pivot index is the index where the sum of all the numbers strictly to the left of the index is equal to the sum of all the numbers strictly to the index's right.\n",
    "\n",
    "If the index is on the left edge of the array, then the left sum is 0 because there are no elements to the left. This also applies to the right edge of the array.\n",
    "\n",
    "Return the leftmost pivot index. If no such index exists, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "63592779-148b-4d45-9dca-85ff30f10ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(n):\n",
    "    for idx,_ in enumerate(n):\n",
    "        tot_sum =sum(n)\n",
    "        if not idx:\n",
    "            l_sum=0\n",
    "            r_sum=tot_sum - n[idx]\n",
    "            if l_sum == r_sum:\n",
    "                return idx\n",
    "        else:\n",
    "            l_sum=sum(n[:idx])\n",
    "            r_sum=tot_sum-n[idx]-l_sum\n",
    "            if l_sum==r_sum:\n",
    "                return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0bf3788d-48fa-465a-98e3-a31321d8358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot([1,7,3,6,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7191fee6-5637-4e0f-af60-b908648a7dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 3, 6, 5]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j[0] for i,j in enumerate(dict.fromkeys([1,7,3,6,5,6]).items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "268f58eb-3868-41b0-84fa-a4a01b941c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(ls,n):\n",
    "    d={}\n",
    "    res=[]\n",
    "    for num in ls:\n",
    "        d[num] = d.get(num,0) + 1\n",
    "    d_sorted = {k:v for k,v in sorted(d.items(), key=lambda x: x[1])}\n",
    "    if n>len(d_sorted):\n",
    "        return d_sorted.keys()\n",
    "    else:\n",
    "        for i,j in enumerate((d_sorted.items())):\n",
    "            if i<n:\n",
    "                res.append(d_sorted[j[0]])\n",
    "    return res, d_sorted, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7a309426-1934-4f6f-8f47-bac32e5ae4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2], {3: 1, 2: 2, 1: 3}, {1: 3, 2: 2, 3: 1})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk([1,1,1,2,2,3], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e54ba-4722-4d3d-8bc6-d35e6110d702",
   "metadata": {},
   "source": [
    "[2,7,11,15], target = 9 Output: [1,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8091e57c-f73e-4fb4-bea0-85ecbfd3d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosum(nums, t):\n",
    "    l_idx, r_idx = 0, len(nums)-1\n",
    "    while l_idx<r_idx:\n",
    "        curr_sum = nums[l_idx] + nums[r_idx]\n",
    "        if curr_sum>t:\n",
    "            r_idx-=1\n",
    "        elif curr_sum<t:\n",
    "            l_idx+=1\n",
    "        else:\n",
    "            return [l_idx+1, r_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "26f9a2a0-9f72-4ef0-948e-1643c53f8882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosum([2,3,4],  6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "b85a46b4-2f17-44e1-be7d-178e07056e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset and split it into training and testing sets\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors and normalize the features\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std\n",
    "\n",
    "# Define the linear classifier model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=4, out_features=3),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "914a0542-6341-4fb7-963b-6d9b1c9a11c8",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mInit signature:\u001b[0m\n",
       "\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0msize_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mignore_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduce\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreduction\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'mean'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mlabel_smoothing\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m     \n",
       "This criterion computes the cross entropy loss between input logits\n",
       "and target.\n",
       "\n",
       "It is useful when training a classification problem with `C` classes.\n",
       "If provided, the optional argument :attr:`weight` should be a 1D `Tensor`\n",
       "assigning weight to each of the classes.\n",
       "This is particularly useful when you have an unbalanced training set.\n",
       "\n",
       "The `input` is expected to contain the unnormalized logits for each class (which do `not` need\n",
       "to be positive or sum to 1, in general).\n",
       "`input` has to be a Tensor of size :math:`(C)` for unbatched input,\n",
       ":math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1` for the\n",
       "`K`-dimensional case. The last being useful for higher dimension inputs, such\n",
       "as computing cross entropy loss per-pixel for 2D images.\n",
       "\n",
       "The `target` that this criterion expects should contain either:\n",
       "\n",
       "- Class indices in the range :math:`[0, C)` where :math:`C` is the number of classes; if\n",
       "  `ignore_index` is specified, this loss also accepts this class index (this index\n",
       "  may not necessarily be in the class range). The unreduced (i.e. with :attr:`reduction`\n",
       "  set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - w_{y_n} \\log \\frac{\\exp(x_{n,y_n})}{\\sum_{c=1}^C \\exp(x_{n,c})}\n",
       "      \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\sum_{n=1}^N \\frac{1}{\\sum_{n=1}^N w_{y_n} \\cdot \\mathbb{1}\\{y_n \\not= \\text{ignore\\_index}\\}} l_n, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       "  Note that this case is equivalent to applying :class:`~torch.nn.LogSoftmax`\n",
       "  on an input, followed by :class:`~torch.nn.NLLLoss`.\n",
       "\n",
       "- Probabilities for each class; useful when labels beyond a single class per minibatch item\n",
       "  are required, such as for blended labels, label smoothing, etc. The unreduced (i.e. with\n",
       "  :attr:`reduction` set to ``'none'``) loss for this case can be described as:\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad\n",
       "      l_n = - \\sum_{c=1}^C w_c \\log \\frac{\\exp(x_{n,c})}{\\sum_{i=1}^C \\exp(x_{n,i})} y_{n,c}\n",
       "\n",
       "  where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight,\n",
       "  :math:`C` is the number of classes, and :math:`N` spans the minibatch dimension as well as\n",
       "  :math:`d_1, ..., d_k` for the `K`-dimensional case. If\n",
       "  :attr:`reduction` is not ``'none'`` (default ``'mean'``), then\n",
       "\n",
       "  .. math::\n",
       "      \\ell(x, y) = \\begin{cases}\n",
       "          \\frac{\\sum_{n=1}^N l_n}{N}, &\n",
       "           \\text{if reduction} = \\text{`mean';}\\\\\n",
       "            \\sum_{n=1}^N l_n,  &\n",
       "            \\text{if reduction} = \\text{`sum'.}\n",
       "        \\end{cases}\n",
       "\n",
       ".. note::\n",
       "    The performance of this criterion is generally better when `target` contains class\n",
       "    indices, as this allows for optimized computation. Consider providing `target` as\n",
       "    class probabilities only when a single class label per minibatch item is too restrictive.\n",
       "\n",
       "Args:\n",
       "    weight (Tensor, optional): a manual rescaling weight given to each class.\n",
       "        If given, has to be a Tensor of size `C`\n",
       "    size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,\n",
       "        the losses are averaged over each loss element in the batch. Note that for\n",
       "        some losses, there are multiple elements per sample. If the field :attr:`size_average`\n",
       "        is set to ``False``, the losses are instead summed for each minibatch. Ignored\n",
       "        when :attr:`reduce` is ``False``. Default: ``True``\n",
       "    ignore_index (int, optional): Specifies a target value that is ignored\n",
       "        and does not contribute to the input gradient. When :attr:`size_average` is\n",
       "        ``True``, the loss is averaged over non-ignored targets. Note that\n",
       "        :attr:`ignore_index` is only applicable when the target contains class indices.\n",
       "    reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the\n",
       "        losses are averaged or summed over observations for each minibatch depending\n",
       "        on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per\n",
       "        batch element instead and ignores :attr:`size_average`. Default: ``True``\n",
       "    reduction (str, optional): Specifies the reduction to apply to the output:\n",
       "        ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will\n",
       "        be applied, ``'mean'``: the weighted mean of the output is taken,\n",
       "        ``'sum'``: the output will be summed. Note: :attr:`size_average`\n",
       "        and :attr:`reduce` are in the process of being deprecated, and in\n",
       "        the meantime, specifying either of those two args will override\n",
       "        :attr:`reduction`. Default: ``'mean'``\n",
       "    label_smoothing (float, optional): A float in [0.0, 1.0]. Specifies the amount\n",
       "        of smoothing when computing the loss, where 0.0 means no smoothing. The targets\n",
       "        become a mixture of the original ground truth and a uniform distribution as described in\n",
       "        `Rethinking the Inception Architecture for Computer Vision <https://arxiv.org/abs/1512.00567>`__. Default: :math:`0.0`.\n",
       "\n",
       "Shape:\n",
       "    - Input: Shape :math:`(C)`, :math:`(N, C)` or :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
       "      in the case of `K`-dimensional loss.\n",
       "    - Target: If containing class indices, shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with\n",
       "      :math:`K \\geq 1` in the case of K-dimensional loss where each value should be between :math:`[0, C)`.\n",
       "      If containing class probabilities, same shape as the input and each value should be between :math:`[0, 1]`.\n",
       "    - Output: If reduction is 'none', shape :math:`()`, :math:`(N)` or :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \\geq 1`\n",
       "      in the case of K-dimensional loss, depending on the shape of the input. Otherwise, scalar.\n",
       "\n",
       "\n",
       "    where:\n",
       "\n",
       "    .. math::\n",
       "        \\begin{aligned}\n",
       "            C ={} & \\text{number of classes} \\\\\n",
       "            N ={} & \\text{batch size} \\\\\n",
       "        \\end{aligned}\n",
       "\n",
       "Examples::\n",
       "\n",
       "    >>> # Example of target with class indices\n",
       "    >>> loss = nn.CrossEntropyLoss()\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.empty(3, dtype=torch.long).random_(5)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "    >>>\n",
       "    >>> # Example of target with class probabilities\n",
       "    >>> input = torch.randn(3, 5, requires_grad=True)\n",
       "    >>> target = torch.randn(3, 5).softmax(dim=1)\n",
       "    >>> output = loss(input, target)\n",
       "    >>> output.backward()\n",
       "\u001b[0;31mInit docstring:\u001b[0m Initializes internal Module state, shared by both nn.Module and ScriptModule.\n",
       "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/envs/py38/lib/python3.8/site-packages/torch/nn/modules/loss.py\n",
       "\u001b[0;31mType:\u001b[0m           type\n",
       "\u001b[0;31mSubclasses:\u001b[0m     \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.nn.CrossEntropyLoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a41cf3a2-1d66-44e6-bec2-c9ecdda1923b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8238, 0.4491, 1.7523, 0.7523])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the loss function and the optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} Loss: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2d561e5c-c234-46cc-9a29-13adf92db761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4693, 0.0385, 0.2544],\n",
       "        [0.5307, 0.9615, 0.7456]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a random tensor\n",
    "a = torch.randn(2, 3)\n",
    "\n",
    "# Apply Softmax\n",
    "F.softmax(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f11c50d1-84dd-460c-8999-702221b66372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2747, -2.0503, -0.2131],\n",
       "        [ 0.3976,  1.1685,  0.8623]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7a87156c-d221-4b0a-b2e4-ab97e6589fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ae210219-a3f8-4d14-b6fb-45fd2e546c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5841928744333489"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0.2747)/sum([math.exp(i) for i in a[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "972af7b2-f501-46c4-9c48-79b50584d2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5842, 0.0571, 0.3587],\n",
       "        [0.2104, 0.4548, 0.3348]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b608a7-3667-4372-8645-688a4639a52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bba8452-af84-4034-9c31-3b35cee78bd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e774879d-1b24-43b0-8677-71763760677e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16cf76b9-4d75-4209-8849-98f665c0d384",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec95f1f7-cd76-433a-9acb-003904c19061",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6811076c-c0c3-4d87-9314-d014718f059a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb0256a-8ed2-4e9b-83af-bc15e1fbdd0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd677a90-8c53-4b80-ac9e-9640c92e2ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35320ea-592a-4ed1-acf2-52cf75429409",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "858c6d1f-ce86-4e1e-b98b-7d89f3f56cad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c9d8cc9-80f5-4971-a25c-ffa5b83ce4df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c21557-1bca-48d5-a6f3-ac744e9fa06d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a907ca5-3df7-4439-acb4-d4fc901e11a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05f16d7-4012-4814-869b-f840286c70c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97414691-54a2-4fa2-a469-2f1d21bdd5e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
