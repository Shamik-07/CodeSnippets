{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fdb644e2-8589-44f1-bb22-d43938cbf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_traveller(m,n, d={}):\n",
    "    key = (m,n)\n",
    "    if m==1 and n==1:\n",
    "        return 1\n",
    "    if m==0 or n==0:\n",
    "        return 0\n",
    "    if key in d:\n",
    "        return d[key]\n",
    "    d[key]=grid_traveller(m-1,n,d) + grid_traveller(m,n-1,d)\n",
    "    return d[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca01adaa-9723-4f7a-b212-d5dc4f5b2312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 1, 2333606220)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_traveller(2,3), grid_traveller(3,2), grid_traveller(1,1), grid_traveller(18, 18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cdb84088-02c2-4483-ac73-0b5bad24d4db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fib(n, d={}):\n",
    "    if n<=2:\n",
    "        return 1\n",
    "    if n in d:\n",
    "        return d[n]\n",
    "    d[n] = fib(n-1, d) + fib(n-2, d)\n",
    "    return d[n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94ceef4c-0807-4ff2-aa1f-fd500e8f2e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fib(3), fib(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b6f0f2ff-d9b8-4a8b-bb90-359e6cf4b9bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_alpha = \"The quick brown foxes jumped over the lazy dog.\"\n",
    "all_alpha = all_alpha.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "6311f06e-e9fb-4e94-8331-5ef08025947f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('The', 'quick', 'brown', 'foxes'),\n",
       " ('quick', 'brown', 'foxes', 'jumped'),\n",
       " ('brown', 'foxes', 'jumped', 'over'),\n",
       " ('foxes', 'jumped', 'over', 'the'),\n",
       " ('jumped', 'over', 'the', 'lazy'),\n",
       " ('over', 'the', 'lazy', 'dog.'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(zip(*[all_alpha[i:] for i in range(4)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23778131-e0b1-4ea6-b31b-3bb3bdafbca1",
   "metadata": {
    "tags": []
   },
   "source": [
    "Given an array of strings, group the anagrams together. You can return the answer in any order.\n",
    "\n",
    "An Anagram is a word or phrase formed by rearranging the letters of a different word or phrase, typically using all the original letters exactly once.\n",
    "\n",
    "Solved using hashmap\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: strs = [\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"] Output: [[\"bat\"],[\"nat\",\"tan\"],[\"ate\",\"eat\",\"tea\"]] Example 2:\n",
    "\n",
    "Input: strs = [\"\"] Output: [[\"\"]] Example 3:\n",
    "\n",
    "Input: strs = [\"a\"] Output: [[\"a\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f37cf21e-a863-47ff-9d58-0d3d298a9067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('a', 'c', 't')"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuple(sorted(\"atc\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "55b148bc-98b7-474d-965a-2fb88c0e7d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ana(strs):\n",
    "    d={}\n",
    "    for eachstr in strs:\n",
    "        d.setdefault(tuple(sorted(eachstr)),[]).append(eachstr)\n",
    "    return d.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "799c4fa2-7d32-4e95-8e8b-086298a7732b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([['eat', 'tea', 'ate'], ['tan', 'nat'], ['bat']])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ana([\"eat\",\"tea\",\"tan\",\"ate\",\"nat\",\"bat\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d373ae-fa60-4422-879e-8aaf5b1ea8b4",
   "metadata": {},
   "source": [
    "Can Sum O(m*n)\n",
    "\n",
    "Given a target sum and a list of integers, verify whether the integers can actually match the target sum, same integers can be reused as many times as needed.\n",
    "\n",
    "Example 1 target = 7 list = [5,4,3,7] output = True\n",
    "\n",
    "Example 2 target = 14 list = [5,5] output = false"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "af719287-7c2d-4dba-9796-80b9e497310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cansum(target, ints, d={}):\n",
    "    if target in d:\n",
    "        return d[target]\n",
    "    if target ==0:\n",
    "        return True\n",
    "    if target < 0:\n",
    "        return False\n",
    "    for i in ints:\n",
    "        rem = target - i\n",
    "        if cansum(rem, ints, d):\n",
    "            d[target] = True\n",
    "            return True\n",
    "    d[target]= False\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "9d68ce73-332a-458b-94fc-83c4ff0ca83d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cansum(7, [5,4,3,7], {}), cansum(7, [2,4], {})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac9428-b448-4818-a90d-646ff228c37e",
   "metadata": {},
   "source": [
    "Given an array of integers nums and an integer target, return indices of the two numbers such that they add up to target.\n",
    "\n",
    "You may assume that each input would have exactly one solution, and you may not use the same element twice.\n",
    "\n",
    "You can return the answer in any order.\n",
    "\n",
    "Solved using hashmap\n",
    "\n",
    "Example 1:\n",
    "\n",
    "Input: nums = [2,7,11,15], target = 9 Output: [0,1] Explanation: Because nums[0] + nums[1] == 9, we return [0, 1]. Example 2:\n",
    "\n",
    "Input: nums = [3,2,4], target = 6 Output: [1,2] Example 3:\n",
    "\n",
    "Input: nums = [3,3], target = 6 Output: [0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "8e4f1f4e-1feb-44c0-91dd-709a01dc6a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosum(ls, t):\n",
    "    d={}\n",
    "    for i,j in enumerate(ls):\n",
    "        rem = t-j\n",
    "        if rem in d:\n",
    "            return [d[rem],i]\n",
    "        d[j]=i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "81947585-d4ff-4380-a9d9-fe9041c128de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosum([3,3],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95747831-465c-41c9-931d-67a6d21c1325",
   "metadata": {},
   "source": [
    "Pivot Index\n",
    "\n",
    "Given an array of integers nums, calculate the pivot index of this array.\n",
    "\n",
    "The pivot index is the index where the sum of all the numbers strictly to the left of the index is equal to the sum of all the numbers strictly to the index's right.\n",
    "\n",
    "If the index is on the left edge of the array, then the left sum is 0 because there are no elements to the left. This also applies to the right edge of the array.\n",
    "\n",
    "Return the leftmost pivot index. If no such index exists, return -1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "63592779-148b-4d45-9dca-85ff30f10ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pivot(n):\n",
    "    for idx,_ in enumerate(n):\n",
    "        tot_sum =sum(n)\n",
    "        if not idx:\n",
    "            l_sum=0\n",
    "            r_sum=tot_sum - n[idx]\n",
    "            if l_sum == r_sum:\n",
    "                return idx\n",
    "        else:\n",
    "            l_sum=sum(n[:idx])\n",
    "            r_sum=tot_sum-n[idx]-l_sum\n",
    "            if l_sum==r_sum:\n",
    "                return idx\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0bf3788d-48fa-465a-98e3-a31321d8358d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pivot([1,7,3,6,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "7191fee6-5637-4e0f-af60-b908648a7dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 7, 3, 6, 5]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[j[0] for i,j in enumerate(dict.fromkeys([1,7,3,6,5,6]).items())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "268f58eb-3868-41b0-84fa-a4a01b941c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk(ls,n):\n",
    "    d={}\n",
    "    res=[]\n",
    "    for num in ls:\n",
    "        d[num] = d.get(num,0) + 1\n",
    "    d_sorted = {k:v for k,v in sorted(d.items(), key=lambda x: x[1])}\n",
    "    if n>len(d_sorted):\n",
    "        return d_sorted.keys()\n",
    "    else:\n",
    "        for i,j in enumerate((d_sorted.items())):\n",
    "            if i<n:\n",
    "                res.append(d_sorted[j[0]])\n",
    "    return res, d_sorted, d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "7a309426-1934-4f6f-8f47-bac32e5ae4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([1, 2], {3: 1, 2: 2, 1: 3}, {1: 3, 2: 2, 3: 1})"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topk([1,1,1,2,2,3], 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e54ba-4722-4d3d-8bc6-d35e6110d702",
   "metadata": {},
   "source": [
    "[2,7,11,15], target = 9 Output: [1,2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "8091e57c-f73e-4fb4-bea0-85ecbfd3d9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def twosum(nums, t):\n",
    "    l_idx, r_idx = 0, len(nums)-1\n",
    "    while l_idx<r_idx:\n",
    "        curr_sum = nums[l_idx] + nums[r_idx]\n",
    "        if curr_sum>t:\n",
    "            r_idx-=1\n",
    "        elif curr_sum<t:\n",
    "            l_idx+=1\n",
    "        else:\n",
    "            return [l_idx+1, r_idx+1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "26f9a2a0-9f72-4ef0-948e-1643c53f8882",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 3]"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twosum([2,3,4],  6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6b257f-c534-4972-a14d-6c0085122333",
   "metadata": {},
   "source": [
    "nums = [-1,0,1,2,-1,-4] Output: [[-1,-1,2],[-1,0,1]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "b005c59c-7ea3-44b6-907b-adb4a1ee6df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threesum(nums):\n",
    "    results=[]\n",
    "    nums.sort()\n",
    "    \n",
    "    for i,j in enumerate(nums):\n",
    "        if i>0 and j==nums[i-1]:\n",
    "            continue\n",
    "        l_idx, r_idx = i+1,len(nums)-1\n",
    "        while l_idx < r_idx:\n",
    "            curr_sum = nums[l_idx] + nums[r_idx] + j\n",
    "            if curr_sum > 0:\n",
    "                r_idx -=1\n",
    "            elif curr_sum < 0:\n",
    "                l_idx +=1\n",
    "            else:\n",
    "                results.append([nums[l_idx], j, nums[r_idx]])\n",
    "                l_idx +=1\n",
    "                while nums[l_idx] == nums[l_idx-1] and l_idx < r_idx:\n",
    "                    l_idx +=1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "025b2587-4123-4ee1-af40-3396ed364aaa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[-1, -1, 2], [0, -1, 1]], [], [])"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "threesum([-1,0,1,2,-1,-4]), threesum([]), threesum([0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213a7ab8-4977-4f26-920a-7dcdfe3298d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cd8867c-c710-4090-a094-3e1ed1e550b6",
   "metadata": {},
   "source": [
    "# Linear Classifier NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d9bfadb-f1b4-440c-ac15-6a5a1828582b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the Iris dataset and split it into training and testing sets\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data,\n",
    "                                                    iris.target,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert the data into PyTorch tensors and normalize the features\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train)\n",
    "y_test = torch.tensor(y_test)\n",
    "\n",
    "mean = X_train.mean(dim=0)\n",
    "std = X_train.std(dim=0)\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "835b3357-3368-4944-9411-c10d3a507236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the linear classifier model\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=4, out_features=3),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b3db153-dc16-43d0-9f4f-97f81932f23e",
   "metadata": {},
   "source": [
    "# Define the linear classifier model with hidden states\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=4, out_features=10),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=10, out_features=3),\n",
    "    torch.nn.Softmax(dim=1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f40378-a4e4-4a47-a421-c629656aec3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a41cf3a2-1d66-44e6-bec2-c9ecdda1923b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Loss: 1.0061719417572021\n",
      "Epoch: 10 Loss: 1.0007625818252563\n",
      "Epoch: 20 Loss: 0.9955267310142517\n",
      "Epoch: 30 Loss: 0.9904457926750183\n",
      "Epoch: 40 Loss: 0.9855024814605713\n",
      "Epoch: 50 Loss: 0.9806820750236511\n",
      "Epoch: 60 Loss: 0.9759709239006042\n",
      "Epoch: 70 Loss: 0.9713577032089233\n",
      "Epoch: 80 Loss: 0.9668325781822205\n",
      "Epoch: 90 Loss: 0.9623873829841614\n"
     ]
    }
   ],
   "source": [
    "# Define the loss function and the optimizer\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):  # Number of epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model(X_train)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = loss_fn(y_pred, y_train)\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch} Loss: {loss.item()}')\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cf134d2-41bb-4eba-99af-828411f03d9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 2, 2, 0, 1, 2, 0, 2, 0, 2,\n",
       "         2, 2, 2, 2, 0, 0]),\n",
       " tensor([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2, 0, 2,\n",
       "         2, 2, 2, 2, 0, 0]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.argmax(model(X_test), dim=1), y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "176e14b2-c118-4078-b9f5-7f0783d45b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3257, 0.3172, 0.3571], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_test)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "2d561e5c-c234-46cc-9a29-13adf92db761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4693, 0.0385, 0.2544],\n",
       "        [0.5307, 0.9615, 0.7456]])"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Create a random tensor\n",
    "a = torch.randn(2, 3)\n",
    "\n",
    "# Apply Softmax\n",
    "F.softmax(a, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "f11c50d1-84dd-460c-8999-702221b66372",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2747, -2.0503, -0.2131],\n",
       "        [ 0.3976,  1.1685,  0.8623]])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "7a87156c-d221-4b0a-b2e4-ab97e6589fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "ae210219-a3f8-4d14-b6fb-45fd2e546c1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5841928744333489"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math.exp(0.2747)/sum([math.exp(i) for i in a[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "972af7b2-f501-46c4-9c48-79b50584d2a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5842, 0.0571, 0.3587],\n",
       "        [0.2104, 0.4548, 0.3348]])"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b608a7-3667-4372-8645-688a4639a52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c197db8-72f5-42b2-918d-648a8b25cb0c",
   "metadata": {},
   "source": [
    "# Linear Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "740583b7-d490-4b04-8b06-6c3ec0366444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9400feb-687f-41de-9644-a1e66b98e8dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset and split it into training and testing sets\n",
    "diabetes = load_diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(diabetes.data,\n",
    "                                                    diabetes.target,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Convert the data into PyTorch tensors\n",
    "X_train = torch.tensor(X_train).float()\n",
    "X_test = torch.tensor(X_test).float()\n",
    "y_train = torch.tensor(y_train).float().view(-1, 1)  # Reshape y_train to have 2 dimensions\n",
    "y_test = torch.tensor(y_test).float().view(-1, 1)    # Reshape y_test to have 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e64fb075-3017-4aeb-a96a-e1ada6f3069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class RegressionNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RegressionNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)  # Input to hidden layer\n",
    "        self.relu = nn.ReLU()                          # Activation function\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size) # Hidden to output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Set the input size, hidden layer size, and output size\n",
    "input_size = diabetes.data.shape[1]  # Number of input features\n",
    "hidden_size = 10                     # Number of neurons in the hidden layer\n",
    "output_size = 1                      # Number of output features (for regression, usually 1)\n",
    "\n",
    "# Create the neural network\n",
    "model = RegressionNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0898df8d-f6cd-4b83-8265-ea2fa3e26e5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b46bba98-cd60-49d2-94f1-072f42fa7bee",
   "metadata": {},
   "source": [
    "# Define the model with a hidden layer using nn.Sequential\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(diabetes.data.shape[1], 10),  # Input to hidden layer\n",
    "    nn.ReLU(),                              # Activation function for the hidden layer\n",
    "    nn.Linear(10, 1)                        # Hidden to output layer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8dd044d9-756a-45d2-993e-e0393a1f4a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)  # Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "93de27cb-e023-4855-9010-3001f677816c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss: 29863.6465\n",
      "Epoch [101/1000], Loss: 18182.9512\n",
      "Epoch [201/1000], Loss: 8555.9854\n",
      "Epoch [301/1000], Loss: 4960.8135\n",
      "Epoch [401/1000], Loss: 3728.3467\n",
      "Epoch [501/1000], Loss: 2987.0435\n",
      "Epoch [601/1000], Loss: 2762.5471\n",
      "Epoch [701/1000], Loss: 2667.9546\n",
      "Epoch [801/1000], Loss: 2621.2632\n",
      "Epoch [901/1000], Loss: 2582.3611\n",
      "Epoch [1001/1000], Loss: 2549.3413\n",
      "Epoch [1101/1000], Loss: 2531.5759\n",
      "Epoch [1201/1000], Loss: 2518.5947\n",
      "Epoch [1301/1000], Loss: 2508.1697\n",
      "Epoch [1401/1000], Loss: 2498.6658\n",
      "Epoch [1501/1000], Loss: 2489.7092\n",
      "Epoch [1601/1000], Loss: 2481.7207\n",
      "Epoch [1701/1000], Loss: 2474.5342\n",
      "Epoch [1801/1000], Loss: 2469.4895\n",
      "Epoch [1901/1000], Loss: 2464.9402\n",
      "Epoch [2001/1000], Loss: 2459.4458\n",
      "Epoch [2101/1000], Loss: 2453.5969\n",
      "Epoch [2201/1000], Loss: 2446.4741\n",
      "Epoch [2301/1000], Loss: 2437.6755\n",
      "Epoch [2401/1000], Loss: 2427.8005\n",
      "Epoch [2501/1000], Loss: 2414.5803\n",
      "Epoch [2601/1000], Loss: 2405.6121\n",
      "Epoch [2701/1000], Loss: 2398.5171\n",
      "Epoch [2801/1000], Loss: 2385.1755\n",
      "Epoch [2901/1000], Loss: 2367.9333\n",
      "Epoch [3001/1000], Loss: 2357.0234\n",
      "Epoch [3101/1000], Loss: 2350.2092\n",
      "Epoch [3201/1000], Loss: 2345.7251\n",
      "Epoch [3301/1000], Loss: 2339.9143\n",
      "Epoch [3401/1000], Loss: 2335.7876\n",
      "Epoch [3501/1000], Loss: 2332.2512\n",
      "Epoch [3601/1000], Loss: 2329.6106\n",
      "Epoch [3701/1000], Loss: 2327.3616\n",
      "Epoch [3801/1000], Loss: 2325.0227\n",
      "Epoch [3901/1000], Loss: 2323.1570\n",
      "Epoch [4001/1000], Loss: 2321.2656\n",
      "Epoch [4101/1000], Loss: 2319.4766\n",
      "Epoch [4201/1000], Loss: 2317.7852\n",
      "Epoch [4301/1000], Loss: 2315.7725\n",
      "Epoch [4401/1000], Loss: 2311.5964\n",
      "Epoch [4501/1000], Loss: 2306.7727\n",
      "Epoch [4601/1000], Loss: 2302.8159\n",
      "Epoch [4701/1000], Loss: 2297.3816\n",
      "Epoch [4801/1000], Loss: 2294.5325\n",
      "Epoch [4901/1000], Loss: 2292.1116\n",
      "Epoch [5001/1000], Loss: 2290.1177\n",
      "Epoch [5101/1000], Loss: 2287.1062\n",
      "Epoch [5201/1000], Loss: 2284.9138\n",
      "Epoch [5301/1000], Loss: 2283.3960\n",
      "Epoch [5401/1000], Loss: 2281.7070\n",
      "Epoch [5501/1000], Loss: 2278.8052\n",
      "Epoch [5601/1000], Loss: 2277.0027\n",
      "Epoch [5701/1000], Loss: 2273.8772\n",
      "Epoch [5801/1000], Loss: 2269.4670\n",
      "Epoch [5901/1000], Loss: 2265.7659\n",
      "Epoch [6001/1000], Loss: 2261.5574\n",
      "Epoch [6101/1000], Loss: 2258.6877\n",
      "Epoch [6201/1000], Loss: 2257.3879\n",
      "Epoch [6301/1000], Loss: 2256.3838\n",
      "Epoch [6401/1000], Loss: 2255.3491\n",
      "Epoch [6501/1000], Loss: 2254.3354\n",
      "Epoch [6601/1000], Loss: 2253.3962\n",
      "Epoch [6701/1000], Loss: 2252.4526\n",
      "Epoch [6801/1000], Loss: 2251.2861\n",
      "Epoch [6901/1000], Loss: 2249.8606\n",
      "Epoch [7001/1000], Loss: 2248.4087\n",
      "Epoch [7101/1000], Loss: 2247.0806\n",
      "Epoch [7201/1000], Loss: 2246.3967\n",
      "Epoch [7301/1000], Loss: 2245.7144\n",
      "Epoch [7401/1000], Loss: 2245.0073\n",
      "Epoch [7501/1000], Loss: 2243.5920\n",
      "Epoch [7601/1000], Loss: 2242.0461\n",
      "Epoch [7701/1000], Loss: 2240.0422\n",
      "Epoch [7801/1000], Loss: 2238.5078\n",
      "Epoch [7901/1000], Loss: 2235.3013\n",
      "Epoch [8001/1000], Loss: 2232.8828\n",
      "Epoch [8101/1000], Loss: 2231.7407\n",
      "Epoch [8201/1000], Loss: 2229.8833\n",
      "Epoch [8301/1000], Loss: 2226.9353\n",
      "Epoch [8401/1000], Loss: 2224.1619\n",
      "Epoch [8501/1000], Loss: 2222.7039\n",
      "Epoch [8601/1000], Loss: 2221.3167\n",
      "Epoch [8701/1000], Loss: 2219.1936\n",
      "Epoch [8801/1000], Loss: 2218.2605\n",
      "Epoch [8901/1000], Loss: 2215.5254\n",
      "Epoch [9001/1000], Loss: 2202.1941\n",
      "Epoch [9101/1000], Loss: 2198.1155\n",
      "Epoch [9201/1000], Loss: 2191.6277\n",
      "Epoch [9301/1000], Loss: 2185.7227\n",
      "Epoch [9401/1000], Loss: 2181.9680\n",
      "Epoch [9501/1000], Loss: 2179.0784\n",
      "Epoch [9601/1000], Loss: 2177.2383\n",
      "Epoch [9701/1000], Loss: 2175.4590\n",
      "Epoch [9801/1000], Loss: 2173.5400\n",
      "Epoch [9901/1000], Loss: 2171.8423\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10000):  # Number of epochs\n",
    "    model.train()\n",
    "    optimizer.zero_grad()   # Clear gradients\n",
    "    outputs = model(X_train) # Forward pass\n",
    "    loss = criterion(outputs, y_train) # Calculate loss\n",
    "    loss.backward()         # Backward pass\n",
    "    optimizer.step()        # Update weights\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f'Epoch [{epoch+1}/1000], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3cdd9f8f-d6b9-430f-8137-3f38be1b1ea5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([215.2087], grad_fn=<ViewBackward0>), tensor([219.]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(X_test[0]), y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "69b37084-0cb7-4bc0-b046-e93a4f31a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5b4fbaa4-b72e-419d-8ee5-f96f6a7c056c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2841.6616"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_true=y_test, y_pred=model(X_test).detach())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4cb8a7-9f6e-4535-a44c-b70566fd390d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4389440-a765-42b3-abc0-8ae9a7f065f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcf98c7-f666-4160-8bac-a64add64c6ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933e281d-5a01-44d0-a7eb-8e4c6933dc56",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
